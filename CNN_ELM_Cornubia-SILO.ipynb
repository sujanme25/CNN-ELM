{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\Users\\SUJAN\\OneDrive\\Documents\\Energies\\EP2\")\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy\n",
    "from sklearn.neural_network import *\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.preprocessing import *\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "import TSMETRICS\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import *\n",
    "import time\n",
    "import optuna\n",
    "from optuna.study import *\n",
    "from optuna.trial import *\n",
    "import math\n",
    "import datetime\n",
    "import PIALL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import initializers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('1D.xlsx',sheet_name='1D')\n",
    "data = df.filter([\"Cornubia\"])\n",
    "df1=data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1\n",
    "lag =range(1,19)\n",
    "\n",
    "for col in df2.columns:\n",
    "    for l in lag:\n",
    "        df2.loc[:,col+\"_\"+str(l)] = df2[col].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A= pd.read_excel('1D.xlsx',sheet_name='SILO_Cornubia')\n",
    "df_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B=df_A.drop(['YYYY-MM-DD'], axis=1)\n",
    "df_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C=pd.concat([df2, df_B], axis=1, join='inner')\n",
    "df_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df_C.dropna()\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "column_names_to_not_normalize = ['Cornubia']\n",
    "column_names_to_normalize = [x for x in list(df6) if x not in column_names_to_not_normalize ]\n",
    "x = df6[column_names_to_normalize].values\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df6.index)\n",
    "df6[column_names_to_normalize] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=df6.iloc[0:3270,0:36]\n",
    "Test=df6.iloc[3270:,0:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=Train.values\n",
    "X1=array[:,1:36]\n",
    "y1=array[:,0]\n",
    "time_steps= 1\n",
    "X_train= np.zeros((X1.shape[0] - time_steps +1, 1, X1.shape[1]))\n",
    "y_train= np.zeros((y1.shape[0] -time_steps +1,))\n",
    "for ix in range(X_train.shape[0]):\n",
    "    for jx in range(time_steps):\n",
    "        X_train[ix, jx, :]= X1[ix +jx, :]\n",
    "    y_train[ix]= y1[ix + time_steps -1]\n",
    "print (X_train.shape, y_train.shape)\n",
    "array3=Test.values\n",
    "X1=array3[:,1:36]\n",
    "y1=array3[:,0]\n",
    "time_steps= 1\n",
    "X_test= np.zeros((X1.shape[0] - time_steps +1, 1, X1.shape[1]))\n",
    "y_test= np.zeros((y1.shape[0] -time_steps +1,))\n",
    "for ix in range(X_test.shape[0]):\n",
    "    for jx in range(time_steps):\n",
    "        X_test[ix, jx, :]= X1[ix +jx, :]\n",
    "    y_test[ix]= y1[ix + time_steps -1]\n",
    "print (X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "from optuna.study import *\n",
    "from optuna.trial import *\n",
    "import math\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "import datetime\n",
    "path_best_model = 'CNNBLSTM_OPTUNA_Cornubia.h5'\n",
    "best_score=0\n",
    "\n",
    "X_train_shape = X_train.shape\n",
    "TimeSteps=X_train_shape[1]\n",
    "Dims=X_train_shape[2]\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def model_learning_curve(history):\n",
    "    \"\"\"\n",
    "    This function plots a training and testing loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loss with Epoch\n",
    "    plt.figure('16,6')\n",
    "    plt.title('Model Loss')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def callbacks():\n",
    "    DNN_lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                         patience=8, verbose=2, factor=0.7,min_delta = 1e-04, cooldown = 0,min_lr =0)\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "                 ModelCheckpoint(filepath='best_weights_CELM_Kalbar_OPT_SILO.h5',verbose=2,\n",
    "                                 monitor='val_loss', save_best_only=True),DNN_lr_reduction]\n",
    "    return callbacks\n",
    "callbacks_list = callbacks()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "        \n",
    "    inputs = Input(shape=(TimeSteps, Dims))\n",
    "    x=Conv1D(\n",
    "            filters=trial.suggest_int('filters1', 10, 1000, 10),\n",
    "            kernel_size=trial.suggest_int('kernel_size',2,20,step=2),\n",
    "            activation=trial.suggest_categorical('activation1', ['relu', 'tanh']),\n",
    "            padding='same',kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x1=Conv1D(\n",
    "            filters=trial.suggest_int('filters2', 10, 800, 10),\n",
    "            kernel_size=1,\n",
    "            activation=trial.suggest_categorical('activation2', ['relu', 'tanh']),\n",
    "            padding='same',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x2=Conv1D(\n",
    "            filters=trial.suggest_int('filters3', 10, 800, 10),\n",
    "            kernel_size=1,\n",
    "            activation=trial.suggest_categorical('activation3', ['relu', 'tanh']),\n",
    "            padding='same',kernel_initializer=\"glorot_uniform\")(x1)\n",
    "\n",
    "    Flatten_=Flatten(data_format=None,name='FLATTEN')(x2)\n",
    "    x3= Dense(10, activation='sigmoid')(Flatten_ )\n",
    "    output = Dense(1, activation='sigmoid')(x3 )\n",
    "    model = Model(inputs=[inputs], outputs=output)   \n",
    "    optimizer_name = trial.suggest_categorical('optimizer_name', ['Adam', 'RMSprop'])\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=trial.suggest_float('learning_rate',  1e-6, 1e-2, log=True))\n",
    "    else:\n",
    "        optimizer = RMSprop(\n",
    "            learning_rate=trial.suggest_float('learning_rate',  1e-6, 1e-2, log=True),\n",
    "            momentum=trial.suggest_float('momentum',  0.1, 0.9, log=True),\n",
    "        )    \n",
    "        \n",
    "    # We compile our model with a sampled learning rate.\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=optimizer,metrics='mse')\n",
    "    history=model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        shuffle=False,\n",
    "        batch_size=trial.suggest_int('batch_size', 1, 20, 1),\n",
    "        epochs=500,\n",
    "        validation_split=0.2,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the test set.\n",
    "    YP = model.predict(X_test)\n",
    "    YT=y_test\n",
    "    score=root_mean_squared_error(YT.flatten(),YP.flatten())\n",
    "    model_learning_curve(history)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"cnn_study_Kalbar_Opt_SILO\"  # unique identifier of the study.\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name=study_name,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, timeout=8*60*60) # Timeout in seconds e.g. timeout=20*60*60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                     patience=8, verbose=2, factor=0.7,min_delta = 1e-04, cooldown = 0,min_lr =0)\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=50),\n",
    "             ModelCheckpoint(filepath='best_weights_CELM_Cornubia_SILO.h5',verbose=2,\n",
    "                             monitor='val_loss', save_best_only=True),DNN_lr_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define CNN Model\n",
    "\n",
    "def generate_cnn():   \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=20,input_shape=(X_train.shape[1],  X_train.shape[2]),kernel_size=10,activation='relu',\n",
    "                     padding='same',kernel_initializer=\"glorot_uniform\"))\n",
    "    model.add(MaxPool1D(pool_size=1))\n",
    "    model.add(Conv1D(filters=18,kernel_size=8,activation='relu',\n",
    "                     padding='same',kernel_initializer=\"glorot_uniform\"))\n",
    "    model.add(MaxPool1D(pool_size=1))\n",
    "    model.add(Conv1D(filters=14,kernel_size=6,activation='relu',\n",
    "                     padding='same',kernel_initializer=\"glorot_uniform\"))\n",
    "    model.add(MaxPool1D(pool_size=1))\n",
    "    model.add(Flatten(data_format=None,name='FLATTEN'))\n",
    "    model.add(Dense(units=10,activation='relu')),\n",
    "    model.add(Dense(units=1,activation='relu')),\n",
    "    optimizer= RMSprop(learning_rate=0.001, momentum=0.009)\n",
    "    print(model.summary())\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics='mse')\n",
    "    model.fit(X_train, y_train, epochs=1000,validation_split=0.2, batch_size=2, verbose=2,callbacks=callbacks)      \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = generate_cnn()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "cnn =load_model('best_weights_CELM_Cornubia_SILO.h5')\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Hidden Layer\n",
    "#The Hidden Layer is the layer between CNN and ELM\n",
    "layer_name = 'FLATTEN'\n",
    "hidden_layer_model = tf.keras.Model(inputs=cnn.input, outputs=cnn.get_layer(layer_name).output)\n",
    "Train= hidden_layer_model.predict(X_train)\n",
    "Test= hidden_layer_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape,Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ELM_REG import ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ELM(k, hidden, X_train, y_train, num_classes=1):\n",
    "    input_length = k\n",
    "    num_hidden_layers = hidden\n",
    "\n",
    "    model = ELM(input_length,\n",
    "                num_hidden_layers,\n",
    "                num_classes)\n",
    "    \n",
    "    model.fit(X_train, y_train, display_time=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ELM(14, 10, Train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YP=prediction\n",
    "YT=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIALL\n",
    "PIALL_Test_ESN=PIALL.PI(YT.flatten(),YP.flatten())\n",
    "PIALL_Test_ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obj(params):\n",
    "    model = ELM(**params,num_input_nodes=14,num_out_units=1)\n",
    "    model.fit(Train, y_train, display_time=False)\n",
    "    y_pred = model(Test)\n",
    "    score = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(score)\n",
    "    return {'loss': score, 'status': STATUS_OK,'Trained_Model': model} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, \n",
    "# epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1\n",
    "space ={\n",
    "\n",
    "        'num_hidden_units': hp.choice('num_hidden_units',np.arange(1,1000 , 5, dtype=int)),\n",
    "        'activation':hp.choice('num_input_nodes', ['sigmoid','hardlimit','fourier'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(trials, space):\n",
    "    \n",
    "    best = fmin(Obj, space, algo=tpe.suggest, max_evals=500)\n",
    "    return best\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "\n",
    "best_params = optimize(trials, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, space_eval,Trials\n",
    "# Return the best parameters\n",
    "best_hp=space_eval(space, best_params)\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ELM(num_input_nodes=14, num_hidden_units=51, num_out_units=1, activation='fourier',\n",
    "                 loss='mse', beta_init=None, w_init=None, bias_init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(Train, y_train, display_time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model2(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YP=prediction\n",
    "YT=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIALL\n",
    "PIALL_Test_CELM=PIALL.PI(YT.flatten(),YP.flatten())\n",
    "PIALL_Test_CELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\SUJANPC\\\\Documents\\\\Energies\\\\EP2\")\n",
    "TEST=pd.DataFrame([YP.flatten(),YT.flatten()]).T\n",
    "TEST.columns=['CELM','YTEST']\n",
    "writer = pd.ExcelWriter('EP2_Cornubia_CELM.xlsx', engine='xlsxwriter')\n",
    "TEST.to_excel(writer, sheet_name='TEST_Result')\n",
    "PIALL_Test_CELM.to_excel(writer, sheet_name='PI')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
