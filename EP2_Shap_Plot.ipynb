{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "os.chdir(\"C:\\\\Users\\\\SUJAN\\\\OneDrive\\\\Documents\\\\Energies\\\\EP2\")\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy\n",
    "from sklearn.neural_network import *\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import *\n",
    "from ngboost import *\n",
    "from sklearn.tree import *\n",
    "from catboost import *\n",
    "from sklearn.preprocessing import *\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "from tensorflow.keras.layers import *\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import mpl_toolkits.axisartist.grid_finder as GF\n",
    "import mpl_toolkits.axisartist.floating_axes as FA\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow\n",
    "os.chdir(\"E:\\\\paper 2022\\\\P9_Electricity Demand\\\\Chart_Metrics_ROugh\")\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy\n",
    "from sklearn.neural_network import *\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import *\n",
    "from ngboost import *\n",
    "from sklearn.tree import *\n",
    "from catboost import *\n",
    "from sklearn.preprocessing import *\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from matplotlib.figure import Figure\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from simple_uq.models.pnn import PNN\n",
    "from simple_uq.util.synthetic_data import create_1d_data\n",
    "from typing import Union, Tuple, List, Any, NoReturn\n",
    "from typing import Union, Tuple, List, Any, NoReturn\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\SUJAN\\\\OneDrive\\\\Documents\\\\Energies\\\\EP2\")\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy\n",
    "from sklearn.neural_network import *\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import *\n",
    "from ngboost import *\n",
    "from sklearn.tree import *\n",
    "from catboost import *\n",
    "from sklearn.preprocessing import *\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "import TSMETRICS\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import *\n",
    "import time\n",
    "import optuna\n",
    "from optuna.study import *\n",
    "from optuna.trial import *\n",
    "import math\n",
    "import datetime\n",
    "from ngboost import *\n",
    "from ngboost.distns import *\n",
    "import PIALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('1D_Chart.xlsx',sheet_name='1D')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.filter([\"Coolum\"])\n",
    "df1=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1\n",
    "lag =range(1,7)\n",
    "\n",
    "for col in df2.columns:\n",
    "    for l in lag:\n",
    "        df2.loc[:,col+\"_\"+str(l)] = df2[col].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A= pd.read_excel('1D_Chart.xlsx',sheet_name='SILO_Coolum')\n",
    "df_B=df_A.drop(['YYYY-MM-DD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C=pd.concat([df2, df_B], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_C\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "column_names_to_not_normalize = ['Coolum']\n",
    "column_names_to_normalize = [x for x in list(df1) if x not in column_names_to_not_normalize ]\n",
    "x = df1[column_names_to_normalize].values\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df1.index)\n",
    "df1[column_names_to_normalize] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df1.dropna()\n",
    "F1=( df5.columns.values)\n",
    "F1\n",
    "df5.columns=['Coolum_MW','Lag1','Lag2','Lag3','Lag4','Lag5','Lag6','Rainfall', 'Tmax',\n",
    "       'Tmin', 'VP', 'VPd', 'Esyn', 'GSR', 'RHmax',\n",
    "       'RHmin', 'Etm', 'MSLP']\n",
    "F2=( df5.columns.values)\n",
    "F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.drop(['Coolum_MW'],axis=1)\n",
    "features_Coolum = ( df6.columns.values)\n",
    "features_Coolum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=df5.iloc[0:2200,0:18]\n",
    "Valid=df5.iloc[2800:3270,0:18]\n",
    "Test=df5.iloc[3270:,0:18]\n",
    "\n",
    "array=Train.values\n",
    "X_train=array[:,1:18]\n",
    "y_train=array[:,0]\n",
    "array3=Test.values\n",
    "X_test=array3[:,1:18]\n",
    "y_test=array3[:,0]\n",
    "array2=Valid.values\n",
    "X_val=array2[:,1:18]\n",
    "y_val=array2[:,0]\n",
    "print (X_test.shape, y_test.shape),print (X_train.shape, y_train.shape),print (X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor\n",
    "CBR_Coolum = CatBoostRegressor(iterations=5000, learning_rate=0.01, loss_function='RMSE',\n",
    "                          verbose=False, random_seed=0)\n",
    "\n",
    "CBR_Coolum.fit(X_train,y_train, eval_set=(X_val,y_val))\n",
    "print(\"best iteration =\", CBR_Coolum.get_best_iteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('1D_Chart.xlsx',sheet_name='1D')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.filter([\"Cornubia\"])\n",
    "df1=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1\n",
    "lag =range(1,6)\n",
    "\n",
    "for col in df2.columns:\n",
    "    for l in lag:\n",
    "        df2.loc[:,col+\"_\"+str(l)] = df2[col].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A= pd.read_excel('1D_Chart.xlsx',sheet_name='SILO_Cornubia')\n",
    "df_B=df_A.drop(['YYYY-MM-DD'], axis=1)\n",
    "df_C=pd.concat([df2, df_B], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_C\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "column_names_to_not_normalize = ['Cornubia']\n",
    "column_names_to_normalize = [x for x in list(df1) if x not in column_names_to_not_normalize ]\n",
    "x = df1[column_names_to_normalize].values\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df1.index)\n",
    "df1[column_names_to_normalize] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df1.dropna()\n",
    "F1=( df5.columns.values)\n",
    "F1\n",
    "df5.columns=['Cornubia','Lag1','Lag2','Lag3','Lag4','Lag5','Rainfall', 'Tmax',\n",
    "       'Tmin', 'VP', 'VPd', 'Esyn', 'GSR', 'RHmax',\n",
    "       'RHmin', 'Etm']\n",
    "F2=( df5.columns.values)\n",
    "F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.drop(['Cornubia'],axis=1)\n",
    "features_Cornubia= ( df6.columns.values)\n",
    "features_Cornubia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=df5.iloc[0:2200,0:16]\n",
    "Valid=df5.iloc[2800:3270,0:16]\n",
    "Test=df5.iloc[3270:,0:16]\n",
    "\n",
    "array=Train.values\n",
    "X_train=array[:,1:16]\n",
    "y_train=array[:,0]\n",
    "array3=Test.values\n",
    "X_test=array3[:,1:16]\n",
    "y_test=array3[:,0]\n",
    "array2=Valid.values\n",
    "X_val=array2[:,1:16]\n",
    "y_val=array2[:,0]\n",
    "print (X_test.shape, y_test.shape),print (X_train.shape, y_train.shape),print (X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor\n",
    "CBR_Cornubia = CatBoostRegressor(iterations=5000, learning_rate=0.01, loss_function='RMSE',\n",
    "                          verbose=False, random_seed=0)\n",
    "\n",
    "CBR_Cornubia.fit(X_train,y_train, eval_set=(X_val,y_val))\n",
    "print(\"best iteration =\", CBR_Cornubia.get_best_iteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('1D_Chart.xlsx',sheet_name='1D')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.filter([\"Caloundra\"])\n",
    "df1=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1\n",
    "lag =range(1,7)\n",
    "\n",
    "for col in df2.columns:\n",
    "    for l in lag:\n",
    "        df2.loc[:,col+\"_\"+str(l)] = df2[col].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A= pd.read_excel('1D_Chart.xlsx',sheet_name='SILO_Caloundra')\n",
    "df_B=df_A.drop(['YYYY-MM-DD'], axis=1)\n",
    "df_C=pd.concat([df2, df_B], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_C\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "column_names_to_not_normalize = ['Caloundra']\n",
    "column_names_to_normalize = [x for x in list(df1) if x not in column_names_to_not_normalize ]\n",
    "x = df1[column_names_to_normalize].values\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df1.index)\n",
    "df1[column_names_to_normalize] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df1.dropna()\n",
    "F1=( df5.columns.values)\n",
    "F1\n",
    "df5.columns=['Caloundra','Lag1','Lag2','Lag3','Lag4','Lag5','Lag6','Rainfall', 'Tmax',\n",
    "       'Tmin', 'VP', 'VPd', 'Esyn', 'GSR', 'RHmax',\n",
    "       'RHmin', 'Etm']\n",
    "F2=( df5.columns.values)\n",
    "F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.drop(['Caloundra'],axis=1)\n",
    "features_Caloundra = ( df6.columns.values)\n",
    "features_Caloundra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=df5.iloc[0:2200,0:18]\n",
    "Valid=df5.iloc[2800:3270,0:18]\n",
    "Test=df5.iloc[3270:,0:18]\n",
    "\n",
    "array=Train.values\n",
    "X_train=array[:,1:18]\n",
    "y_train=array[:,0]\n",
    "array3=Test.values\n",
    "X_test=array3[:,1:18]\n",
    "y_test=array3[:,0]\n",
    "array2=Valid.values\n",
    "X_val=array2[:,1:18]\n",
    "y_val=array2[:,0]\n",
    "print (X_test.shape, y_test.shape),print (X_train.shape, y_train.shape),print (X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor\n",
    "CBR_Caloundra = CatBoostRegressor(iterations=5000, learning_rate=0.01, loss_function='RMSE',\n",
    "                          verbose=False, random_seed=0)\n",
    "\n",
    "CBR_Caloundra.fit(X_train,y_train, eval_set=(X_val,y_val))\n",
    "print(\"best iteration =\", CBR_Caloundra.get_best_iteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('1D_Chart.xlsx',sheet_name='1D')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.filter([\"Lawnton\"])\n",
    "df1=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1\n",
    "lag =range(1,6)\n",
    "\n",
    "for col in df2.columns:\n",
    "    for l in lag:\n",
    "        df2.loc[:,col+\"_\"+str(l)] = df2[col].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A= pd.read_excel('1D_Chart.xlsx',sheet_name='SILO_Lawnton')\n",
    "df_B=df_A.drop(['YYYY-MM-DD'], axis=1)\n",
    "df_C=pd.concat([df2, df_B], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_C\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "column_names_to_not_normalize = ['Lawnton']\n",
    "column_names_to_normalize = [x for x in list(df1) if x not in column_names_to_not_normalize ]\n",
    "x = df1[column_names_to_normalize].values\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df1.index)\n",
    "df1[column_names_to_normalize] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df1.dropna()\n",
    "F1=( df5.columns.values)\n",
    "F1\n",
    "df5.columns=['Lawnton','Lag1','Lag2','Lag3','Lag4','Lag5','Rainfall', 'Tmax',\n",
    "       'Tmin', 'VP', 'VPd', 'Esyn', 'GSR', 'RHmax',\n",
    "       'RHmin', 'Etm']\n",
    "F2=( df5.columns.values)\n",
    "F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.drop(['Lawnton'],axis=1)\n",
    "features_Lawnton = ( df6.columns.values)\n",
    "features_Lawnton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=df5.iloc[0:2500,0:15]\n",
    "Valid=df5.iloc[2800:3270,0:15]\n",
    "Test=df5.iloc[3270:,0:15]\n",
    "\n",
    "array=Train.values\n",
    "X_train=array[:,1:15]\n",
    "y_train=array[:,0]\n",
    "array3=Test.values\n",
    "X_test=array3[:,1:15]\n",
    "y_test=array3[:,0]\n",
    "array2=Valid.values\n",
    "X_val=array2[:,1:15]\n",
    "y_val=array2[:,0]\n",
    "print (X_test.shape, y_test.shape),print (X_train.shape, y_train.shape),print (X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor\n",
    "CBR_Lawnton = CatBoostRegressor(iterations=5000, learning_rate=0.01, loss_function='RMSE',\n",
    "                          verbose=False, random_seed=0)\n",
    "\n",
    "CBR_Lawnton.fit(X_train,y_train, eval_set=(X_val,y_val))\n",
    "print(\"best iteration =\", CBR_Lawnton.get_best_iteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBR_Coolum.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc,rcParams\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "fig=plt.figure(figsize=(14, 12), dpi= 200, facecolor='w', edgecolor='k')\n",
    "ax1 = fig.add_subplot(223) # Create matplotlib axes \n",
    "num_features=14\n",
    "importance = CBR_Coolum.feature_importances_/sum(CBR_Coolum.feature_importances_)\n",
    "idx = np.argsort(importance)[::-1][:num_features]\n",
    "# Primary axis\n",
    "ax1.bar(x = np.arange(0, num_features), height = importance[idx])\n",
    "ax1.set_xticks(ticks = np.arange(0, num_features))\n",
    "ax1.set_xticklabels(features_Coolum[idx], rotation = 90) #, rotation_mode=\"anchor\")\n",
    "ax1.set_xlabel(\"Features\",fontsize=16,fontweight='bold')\n",
    "ax1.set_ylabel(\"Importance\",fontsize=16,fontweight='bold')\n",
    "ax1.xaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "# Secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.cumsum(importance[idx]), 'ro-')\n",
    "#ax2.set_ylabel(\"Accumulative Importance\")\n",
    "\n",
    "\n",
    "\n",
    "num_features=14\n",
    "ax3 = fig.add_subplot(222) # Create matplotlib axes \n",
    "importance = CBR_Cornubia.feature_importances_/sum(CBR_Cornubia.feature_importances_)\n",
    "idx = np.argsort(importance)[::-1][:num_features]\n",
    "\n",
    "# Primary axis\n",
    "ax3.bar(x = np.arange(0, num_features), height = importance[idx])\n",
    "ax3.set_xticks(ticks = np.arange(0, num_features))\n",
    "ax3.set_xticklabels(features_BP[idx], rotation = 90) #, rotation_mode=\"anchor\")\n",
    "#ax3.set_xlabel(\"Features\")\n",
    "#ax3.set_ylabel(\"Importance\")\n",
    "ax3.xaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "# Secondary axis\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(np.cumsum(importance[idx]), 'ro-')\n",
    "ax4.set_ylabel(\"Accumulative Importance\",fontsize=16,fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "num_features=14\n",
    "ax5 = fig.add_subplot(221) # Create matplotlib axes \n",
    "importance = CBR_Caloundra.feature_importances_/sum(CBR_Caloundra.feature_importances_)\n",
    "idx = np.argsort(importance)[::-1][:num_features]\n",
    "\n",
    "\n",
    "# Primary axis\n",
    "ax5.bar(x = np.arange(0, num_features), height = importance[idx])\n",
    "ax5.set_xticks(ticks = np.arange(0, num_features))\n",
    "ax5.set_xticklabels(features_Caloundra[idx], rotation = 90) #, rotation_mode=\"anchor\")\n",
    "#ax5.set_xlabel(\"Features\")\n",
    "ax5.set_ylabel(\"Importance\",fontsize=16,fontweight='bold')\n",
    "ax5.xaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "# Secondary axis\n",
    "ax6 = ax5.twinx()\n",
    "ax6.plot(np.cumsum(importance[idx]), 'ro-')\n",
    "#ax6.set_ylabel(\"Accumulative Importance\")\n",
    "\n",
    "num_features=14\n",
    "ax7 = fig.add_subplot(224) # Create matplotlib axes \n",
    "importance = CBR_Lawnton.feature_importances_/sum(CBR_Lawnton.feature_importances_)\n",
    "idx = np.argsort(importance)[::-1][:num_features]\n",
    "\n",
    "# Primary axis\n",
    "ax7.bar(x = np.arange(0, num_features), height = importance[idx])\n",
    "ax7.set_xticks(ticks = np.arange(0, num_features))\n",
    "ax7.set_xticklabels(features_Lawnton[idx], rotation = 90) #, rotation_mode=\"anchor\")\n",
    "ax7.set_xlabel(\"Features\",fontsize=16,fontweight='bold')\n",
    "#ax7.set_ylabel(\"Importance\")\n",
    "#ax7.set_title('Important Features')\n",
    "ax7.xaxis.set_tick_params(labelsize=16)\n",
    "# Secondary axis\n",
    "ax8 = ax7.twinx()\n",
    "ax8.plot(np.cumsum(importance[idx]), 'ro-')\n",
    "ax8.set_ylabel(\"Accumulative Importance\",fontsize=16,fontweight='bold')\n",
    "sns.despine(top=True,right=False)\n",
    "\n",
    "ax1.set_ylim(0, 0.6);ax2.set_ylim(0.25, 1.05);\n",
    "\n",
    "ax3.set_ylim(0, 0.6);ax4.set_ylim(0.25, 1.05);\n",
    "\n",
    "ax5.set_ylim(0, 0.6);ax6.set_ylim(0.25, 1.05);\n",
    "\n",
    "ax7.set_ylim(0, 0.6);ax8.set_ylim(0.25, 1.05);\n",
    "\n",
    "ax2.set_yticks([]);ax3.set_yticks([]);ax6.set_yticks([]);ax7.set_yticks([]);\n",
    "\n",
    "ax1.text(0.0,0.6, r'c) Coolum Sub-station',fontsize=16,fontweight='bold')\n",
    "ax3.text(0.0,0.6, r'b) Cornubia Sub-station',fontsize=16,fontweight='bold')\n",
    "ax5.text(0.0,0.6, r'a) Caloundra Sub-station',fontsize=16,fontweight='bold')\n",
    "ax7.text(0.0,0.6, r'd) Lawnton Sub-station',fontsize=16,fontweight='bold')\n",
    "\n",
    "for a in [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]:\n",
    "    for label in (a.get_xticklabels() + a.get_yticklabels()):\n",
    "        label.set_fontsize(14)\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "fig.subplots_adjust(hspace=0.4,wspace=0.6)\n",
    "plt.savefig(\"Feature_IMPORTANCE_TNET.JPG\", dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import *\n",
    "import shap\n",
    "shap.initjs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_ready_df = df5.dropna() \n",
    "\n",
    "features = [feat for feat in list(catboost_ready_df) \n",
    "            if feat != 'Lawnton']\n",
    "categorical_features = np.where(catboost_ready_df[features].dtypes != np.float)[0]\n",
    " \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df5[features], \n",
    "                                                    df5[['Lawnton']], \n",
    "                                                    test_size=0.3, \n",
    "                                                     random_state=1)\n",
    "params = {'iterations':5000,\n",
    "        'learning_rate':0.01,\n",
    "        'cat_features':categorical_features,\n",
    "        'depth':6,\n",
    "        'eval_metric':'R2',\n",
    "        'verbose':200,\n",
    "        'od_type':\"Iter\", # overfit detector\n",
    "        'od_wait':5000, # most recent best iteration to wait before stopping\n",
    "        'random_seed': 1\n",
    "          }\n",
    "\n",
    "cat_model = CatBoostRegressor(**params)\n",
    "cat_model.fit(X_train, y_train,   \n",
    "          eval_set=(X_test, y_test), \n",
    "          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n",
    "          plot=True  \n",
    "         );\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "shap_values = cat_model.get_feature_importance(Pool(X_test, label=y_test,cat_features=categorical_features) ,\n",
    "                                               type=\"ShapValues\")\n",
    "expected_value = shap_values[0,-1]\n",
    "shap_values = shap_values[:,:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test,show=False,color_bar=True,plot_type='dot')\n",
    "plt.gcf().axes[-1].set_aspect('auto')\n",
    "plt.gcf().axes[-1].set_box_aspect(20) \n",
    "plt.title(\"d) Lawnton Sub-Station\")\n",
    "plt.ylabel(\"SHAP value for the Predictor Variables\")\n",
    "plt.xlabel(\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 10), dpi= 200, facecolor='w', edgecolor='k')\n",
    "fig.add_subplot(211) # Create matplotlib axes\n",
    "shap.summary_plot(shap_values, X_test,show=False,color_bar=True,plot_type='dot')\n",
    "plt.xlabel('')\n",
    "plt.gcf().axes[-1].set_aspect('auto')\n",
    "#plt.gcf().axes[-1].set_box_aspect(50) \n",
    "fig.add_subplot(212) # Create matplotlib axes\n",
    "shap.summary_plot(shap_values, X_test,show=False,color_bar=True,plot_type='dot')\n",
    "plt.xlabel('')\n",
    "plt.gcf().axes[-1].set_aspect('auto')\n",
    "#plt.gcf().axes[-1].set_box_aspect(50) \n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
